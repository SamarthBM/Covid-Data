{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import findspark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "findspark.init('/home/samarth/spark/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from pyspark.sql.functions import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = spark.read.option(\"inferSchema\",True).csv(\"hdfs://localhost:9000/CovidData\", header=True)\n",
    "df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "covid_data = df.select(\"date\",\"new_cases\",\"new_deaths\",\"new_tests\",\"people_vaccinated\",\"people_fully_vaccinated\")\n",
    "covid_data.createOrReplaceTempView(\"view1\")\n",
    "covid_data.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+----------+---------+-----------------+-----------------------+\n",
      "|    date|new_cases|new_deaths|new_tests|people_vaccinated|people_fully_vaccinated|\n",
      "+--------+---------+----------+---------+-----------------+-----------------------+\n",
      "|30/01/20|        1|      null|     null|             null|                   null|\n",
      "|31/01/20|        0|      null|     null|             null|                   null|\n",
      "|01/02/20|        0|      null|     null|             null|                   null|\n",
      "|02/02/20|        1|      null|     null|             null|                   null|\n",
      "|03/02/20|        1|      null|     null|             null|                   null|\n",
      "|04/02/20|        0|      null|     null|             null|                   null|\n",
      "|05/02/20|        0|      null|     null|             null|                   null|\n",
      "|06/02/20|        0|      null|     null|             null|                   null|\n",
      "|07/02/20|        0|      null|     null|             null|                   null|\n",
      "|08/02/20|        0|      null|     null|             null|                   null|\n",
      "|09/02/20|        0|      null|     null|             null|                   null|\n",
      "|10/02/20|        0|      null|     null|             null|                   null|\n",
      "|11/02/20|        0|      null|     null|             null|                   null|\n",
      "|12/02/20|        0|      null|     null|             null|                   null|\n",
      "|13/02/20|        0|      null|     null|             null|                   null|\n",
      "|14/02/20|        0|      null|     null|             null|                   null|\n",
      "|15/02/20|        0|      null|     null|             null|                   null|\n",
      "|16/02/20|        0|      null|     null|             null|                   null|\n",
      "|17/02/20|        0|      null|     null|             null|                   null|\n",
      "|18/02/20|        0|      null|     null|             null|                   null|\n",
      "+--------+---------+----------+---------+-----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "spark.sql(\"select date,new_cases from view1 where new_cases = 0\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+\n",
      "|    date|new_cases|\n",
      "+--------+---------+\n",
      "|31/01/20|        0|\n",
      "|01/02/20|        0|\n",
      "|04/02/20|        0|\n",
      "|05/02/20|        0|\n",
      "|06/02/20|        0|\n",
      "|07/02/20|        0|\n",
      "|08/02/20|        0|\n",
      "|09/02/20|        0|\n",
      "|10/02/20|        0|\n",
      "|11/02/20|        0|\n",
      "|12/02/20|        0|\n",
      "|13/02/20|        0|\n",
      "|14/02/20|        0|\n",
      "|15/02/20|        0|\n",
      "|16/02/20|        0|\n",
      "|17/02/20|        0|\n",
      "|18/02/20|        0|\n",
      "|19/02/20|        0|\n",
      "|20/02/20|        0|\n",
      "|21/02/20|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "spark.sql(\"select date,new_cases from view1 where new_cases >= 50\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+\n",
      "|    date|new_cases|\n",
      "+--------+---------+\n",
      "|20/03/20|       50|\n",
      "|21/03/20|       86|\n",
      "|22/03/20|       66|\n",
      "|23/03/20|      103|\n",
      "|25/03/20|      121|\n",
      "|26/03/20|       70|\n",
      "|27/03/20|      160|\n",
      "|28/03/20|      100|\n",
      "|30/03/20|      227|\n",
      "|31/03/20|      146|\n",
      "|01/04/20|      601|\n",
      "|02/04/20|      545|\n",
      "|04/04/20|      515|\n",
      "|05/04/20|      506|\n",
      "|06/04/20|     1190|\n",
      "|07/04/20|      533|\n",
      "|08/04/20|      605|\n",
      "|09/04/20|      809|\n",
      "|10/04/20|      873|\n",
      "|11/04/20|      848|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "spark.sql(\"select date,new_cases from view1 where new_cases=(select MAX(new_cases) from view1)\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+\n",
      "|    date|new_cases|\n",
      "+--------+---------+\n",
      "|16/09/20|    97894|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "spark.sql(\"select SUM(new_cases) as total_cases from view1\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+\n",
      "|total_cases|\n",
      "+-----------+\n",
      "|   11846652|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "spark.sql(\"select date,new_cases,new_deaths from view1 where new_cases >= 100 and new_deaths >= 100\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+----------+\n",
      "|    date|new_cases|new_deaths|\n",
      "+--------+---------+----------+\n",
      "|02/05/20|     2442|       100|\n",
      "|04/05/20|     3932|       175|\n",
      "|05/05/20|     2963|       127|\n",
      "|07/05/20|     3364|       104|\n",
      "|09/05/20|     3113|       116|\n",
      "|10/05/20|     4353|       111|\n",
      "|12/05/20|     3524|       121|\n",
      "|13/05/20|     3763|       136|\n",
      "|15/05/20|     3787|       104|\n",
      "|16/05/20|     4864|       118|\n",
      "|17/05/20|     5050|       154|\n",
      "|18/05/20|     4630|       131|\n",
      "|19/05/20|     6147|       146|\n",
      "|20/05/20|     5553|       132|\n",
      "|21/05/20|     6198|       150|\n",
      "|22/05/20|     6568|       142|\n",
      "|23/05/20|     6629|       142|\n",
      "|24/05/20|     7113|       156|\n",
      "|25/05/20|     6414|       148|\n",
      "|26/05/20|     5843|       172|\n",
      "+--------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "spark.sql(\"select SUM(new_deaths) as total_deaths from view1\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+\n",
      "|total_deaths|\n",
      "+------------+\n",
      "|      160949|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "spark.sql(\"select date,new_deaths from view1 where new_deaths=(select MAX(new_deaths) from view1)\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+----------+\n",
      "|    date|new_deaths|\n",
      "+--------+----------+\n",
      "|16/06/20|      2003|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "pos_rate=spark.sql(\"select date,((new_cases)/(new_tests)*100) as positivity_rate from view1\")\n",
    "pos_rate.createOrReplaceTempView(\"positive_rate\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "spark.sql(\"select date,positivity_rate from positive_rate where positivity_rate >= 0\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-------------------+\n",
      "|    date|    positivity_rate|\n",
      "+--------+-------------------+\n",
      "|19/03/20| 19.895287958115183|\n",
      "|20/03/20|  4.716981132075472|\n",
      "|21/03/20|  6.490566037735849|\n",
      "|22/03/20|  5.084745762711865|\n",
      "|23/03/20| 2.7777777777777777|\n",
      "|24/03/20|  23.56687898089172|\n",
      "|25/03/20| 2.8271028037383177|\n",
      "|03/04/20|0.11270780501549732|\n",
      "|04/04/20|  4.810836057916861|\n",
      "|05/04/20|  5.279632721202003|\n",
      "|06/04/20| 10.317322698109937|\n",
      "|07/04/20|  4.116783810921449|\n",
      "|08/04/20|  4.351265822784811|\n",
      "|09/04/20|  4.761344241068801|\n",
      "|10/04/20|  5.316686967113276|\n",
      "|11/04/20|  4.699623143427178|\n",
      "|12/04/20|  4.635397581531697|\n",
      "|13/04/20|   5.72319545079336|\n",
      "|14/04/20|  3.782142726507919|\n",
      "|15/04/20|  2.810879956911062|\n",
      "+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "spark.sql(\"select MEAN(positivity_rate) as average_positive_rate from positive_rate\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------+\n",
      "|average_positive_rate|\n",
      "+---------------------+\n",
      "|    5.585317946404814|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "spark.sql(\"select date,positivity_rate from positive_rate where positivity_rate = (select MAX(positivity_rate) from positive_rate ) \").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----------------+\n",
      "|    date|  positivity_rate|\n",
      "+--------+-----------------+\n",
      "|24/03/20|23.56687898089172|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "deathrate=spark.sql(\"select date,((new_deaths)/(new_cases)*100) as death_rate from view1\")\n",
    "deathrate.createOrReplaceTempView(\"death\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "spark.sql(\"select date,death_rate from death where death_rate >=0\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------------------+\n",
      "|    date|        death_rate|\n",
      "+--------+------------------+\n",
      "|11/03/20|16.666666666666664|\n",
      "|12/03/20|               0.0|\n",
      "|13/03/20| 11.11111111111111|\n",
      "|14/03/20|               0.0|\n",
      "|15/03/20|               0.0|\n",
      "|16/03/20|               0.0|\n",
      "|17/03/20|4.3478260869565215|\n",
      "|18/03/20|               0.0|\n",
      "|19/03/20| 2.631578947368421|\n",
      "|20/03/20|               2.0|\n",
      "|22/03/20| 4.545454545454546|\n",
      "|23/03/20| 2.912621359223301|\n",
      "|24/03/20|               0.0|\n",
      "|25/03/20|1.6528925619834711|\n",
      "|26/03/20|11.428571428571429|\n",
      "|27/03/20|               0.0|\n",
      "|28/03/20|               4.0|\n",
      "|29/03/20| 8.108108108108109|\n",
      "|30/03/20|2.2026431718061676|\n",
      "|31/03/20| 2.054794520547945|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "spark.sql(\"select MEAN(death_rate) as Average_death_rate from death\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+\n",
      "|Average_death_rate|\n",
      "+------------------+\n",
      "|1.8005521985084911|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "spark.sql(\"select date,death_rate from death where death_rate = (select MAX(death_rate) from death)\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+-----------------+\n",
      "|    date|       death_rate|\n",
      "+--------+-----------------+\n",
      "|16/06/20|18.25223254966284|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "spark.sql(\"select SUM(people_vaccinated) as first_dose_vaccination from view1\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+\n",
      "|first_dose_vaccination|\n",
      "+----------------------+\n",
      "|             910537195|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "spark.sql(\"select SUM(people_fully_vaccinated) as second_dose_vaccination from view1\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------+\n",
      "|second_dose_vaccination|\n",
      "+-----------------------+\n",
      "|              153359426|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}